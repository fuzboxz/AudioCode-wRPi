My goal with the generative system was to get familiar with what we learnt during the first three session of the course. I wanted to utilize the clock, substractive synthesis and the delay. As I had a hard time understanding the relationship between miliseconds and tempo, I aimed to use this more heavily in my music. I also wanted the application to create a musical structure using simple mathematic expressions instead of hardcoding values.
The application consists of a running clock which triggers a function which handles all the instruments. To have better control and visibility on the different instruments, I used a separate function for all of them instead of a single gigantic wall-of-code. Whether an instrument should trigger depends on the song position and/or a random generator. To separate each instrument into its own function, all the functions rely on the shared AudioContext reference. Currently, all of the instruments destroy themselves after their envelope ends, except the tape delay, which should be theoretically shared through all the instruments as a send type of effect. I did not take care of this as only the pluck uses this effect and it seems to have no short term effect on the hardware to generate a new tape delay every time it is used, but once we move towards less computationally powerful hardware, this should be fixed.
The composition currently uses D# minor with a tempo of 167 BPM. This choice was arbitrary and can easily be changed as I use a constant for both the tempo and the root note, which can be easily modified in the source code. No hardcoded values were used, all of the pitch and tempo information relies on the constant variables, with random note shifts in the minor scale. Lowering the tempo too much could however make the sounds overlap each other as the envelopes don’t rely on tempo information and the values are hardcoded. To keep up the value ratio between envelopes, a common value could be introduced which handles the scaling of the envelope values based on the tempo information.
As the melody was really chaotic using seven notes from the scale, I decided to use only 5, which lead to a less harsh, calmer sound. Using a note probability table to decide which note should be played would make a step-up compared to this solution, but I decided that that's too much for this project.
The instruments used in the song all use substractive synthesis: the kick uses a sine wave with a frequency envelope, a snare that uses a mixture of noise and sinewaves, a hat and crash that relies on noise with different envelope length, a bass that uses 3 detuned oscillators filtered, a pluck with a squarewave and a tape delay and a supersaw inspired string section. 
These instruments all use basic sound design techniques which I learned during CalArts’s Sound Design with Reaktor course. Most of the applications learned from Reaktor can be easily applied to HappyBrackets/Beads, which makes it really easy to learn about audio programming using existing knowledge.
To record the composition, a delay trigger was used that records for 128 beats, stops itself and exits the proccess.
